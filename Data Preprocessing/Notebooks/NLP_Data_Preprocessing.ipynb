{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaeVhLPPWO-w",
        "outputId": "14606096-2a5f-4e69-e0ce-df3702354f44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.4/320.4 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Error loading stem_porter: Package 'stem_porter' not found\n",
            "[nltk_data]     in index\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "!pip install contractions -q\n",
        "!pip install textacy -q #based on spacy\n",
        "\n",
        "import contractions\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "nltk.download('punkt') #Dictionary for punctuation\n",
        "nltk.download('averaged_perceptron_tagger') #for tokenization\n",
        "nltk.download('stopwords')\n",
        "nltk.download('stem_porter')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from textacy import preprocessing\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from textacy import preprocessing\n",
        "import contractions\n",
        "\n",
        "# Ensure necessary NLTK data is downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def f_preprocess_data(preprocess_text):\n",
        "    \"\"\"\n",
        "    Preprocesses the given text by applying cleaning, normalization, and tokenization steps,\n",
        "    including removal of URLs, specific sequences like '&amp;#x200B;', and unusual characters.\n",
        "\n",
        "    Parameters:\n",
        "    preprocess_text (str): The text to preprocess.\n",
        "\n",
        "    Returns:\n",
        "    str: The preprocessed text.\n",
        "    \"\"\"\n",
        "    if pd.isnull(preprocess_text):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove URLs\n",
        "    preprocess_text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', preprocess_text)\n",
        "\n",
        "    # Remove specific sequences like '&amp;#x200B;'\n",
        "    preprocess_text = preprocess_text.replace('&amp;#x200B;', '')\n",
        "\n",
        "    # Handling contractions and specific patterns before tokenization\n",
        "    preprocess_text = contractions.fix(preprocess_text, slang=False)\n",
        "    preprocess_text = preprocessing.replace.hashtags(preprocess_text, repl=\"_hashtag_\")\n",
        "    preprocess_text = preprocessing.replace.user_handles(preprocess_text, repl=\"_user_\")\n",
        "    preprocess_text = preprocessing.replace.emojis(preprocess_text, repl=\"_emoji_\")\n",
        "    preprocess_text = preprocessing.replace.urls(preprocess_text, repl=\"_url_\")\n",
        "\n",
        "    # Replace or remove unusual characters (keeping only letters, numbers, basic punctuation, and whitespace)\n",
        "    preprocess_text = re.sub(r'[^a-zA-Z0-9\\s.,!?-]', '', preprocess_text)\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(preprocess_text)\n",
        "\n",
        "    # Discard texts shorter than 5 words\n",
        "    if len(tokens) < 5:\n",
        "        return \"Insufficient context\"  # or return \"\" to indicate no useful content\n",
        "\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "    tokens = [lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "    tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens]\n",
        "    tokens = [token for token in tokens if token != '']\n",
        "\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOOdHxrvW1u2",
        "outputId": "4732fc60-ed5b-45a6-c7e3-18f64c770d0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "fileIn = \"/content/2 Anxiety_April2019 - MSDS-NLP 2024.xlsx\"\n",
        "\n",
        "df = pd.read_excel( fileIn, index_col=None )\n",
        "\n",
        "df.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fF3sZSYW5Se",
        "outputId": "113a348d-28ae-4c55-f2f7-22ef630d1442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['subreddit', 'id', 'title', 'date', 'selftext'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"preprocess_text\"] = df[\"selftext\"].apply(f_preprocess_data)"
      ],
      "metadata": {
        "id": "Yq4pKS00W8ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the concatenated DataFrame back to the Excel file\n",
        "fileOut = \"Preprocessed_Anxiety_Data_23MSDS15.xlsx\"\n",
        "df.to_excel(fileOut, index=False)"
      ],
      "metadata": {
        "id": "ROSzAF6CXA_O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
